# A Snapshot of the Current Task-Space for Visually Rich Documents

### [Ritesh Sarkhel](https://sarkhelritesh.github.io/)

<br/>

![Sample Visually Rich Documents](https://github.com/sarkhelritesh/vrd_resource/blob/main/github_vrd_example.jpg?raw=true "Sample Visually Rich Documents")

A lot of interesting works have been published on Visually Rich Documents (VRD) in recent years. What makes tasks defined on a VRD often more challenging than its plain-text counterpart is its multimodal nature. We provide a list of works on various problems related to VRD here. Our hope is that this will act as a *resource of reference* for researchers interested in this domain. 

The following list presents, in no particular order, a number of intersting papers on VRD published in recent years. Expand each entry to reveal a brief description of its main constributions. You will also find a single bibtex file (```vrd.bib```) containing all the papers listed in this repository.

<br/>

> **Disclaimer:** Please note that this is \*\*not\*\* an exhaustive list. If you think a work should feature on this list, please submit a pull request or [email me](mailto:sarkhel.5@osu.edu). We look forward to your contribution to keep this page up-to-date and useful.

<br/>

# Content Overview

We list papers from five different directions below. 

- [Information Extraction](#information-extraction)
- [Language Model](#language-model)
- [Layout Analysis](#layout-analysis)
- [Classification](#document-classification)
- [Resource Papers](#resource-papers)

<br/>

# Information Extraction

1. [Fonduer: Knowledge base construction from richly formatted data](https://dl.acm.org/doi/pdf/10.1145/3183713.3183729), Sen Wu, Luke Hsiao, Xiao Cheng, Braden Hancock, Theodoros Rekatsinas, Philip Levis, and Christopher Ré ```SIGMOD 2018``` <details> <summary> + </summary> ✍ *"We focus on knowledge base construction (KBC) from richly formatted data. In contrast to KBC from text or tabular data, KBC from richly formatted data aims to extract relations conveyed jointly via textual, structural, tabular, and visual expressions. We introduce Fonduer, a machine-learning-based KBC system for richly formatted data. Fonduer presents a new data model that accounts for three challenging characteristics of richly formatted data: (1) prevalent document-level relations, (2) multimodality, and (3) data variety. Fonduer uses a new deep-learning model to automatically capture the representation (i.e., features) needed to learn how to extract relations from richly formatted data. Finally, Fonduer provides a new programming model that enables users to convert domain expertise, based on multiple modalities of information, to meaningful signals of supervision for training a KBC system. "* </details> 
2. [Visual segmentation for information extraction from heterogeneous visually rich documents](https://dl.acm.org/doi/pdf/10.1145/3299869.3319867), Ritesh Sarkhel, Arnab Nandi  ```SIGMOD 2019``` <details> <summary> + </summary> ✍ *" We propose VS2, a generalized approach for information extraction from heterogeneous visually rich documents. There are two major contributions of this work. First, we propose a robust segmentation algorithm that decomposes a visually rich document into a bag of visually isolated but semantically coherent areas, called logical blocks. Document type agnostic low-level visual and semantic features are used in this process. Our second contribution is a distantly supervised search-and-select method for identifying the named entities within these documents by utilizing the context boundaries defined by these logical blocks."* </details>
5. [Improving Information Extraction from Visually Rich Documents using Visual Span Representations](https://dl.acm.org/doi/pdf/10.14778/3446095.3446104), Ritesh Sarkhel, Arnab Nandi  ```VLDB 2021``` <details> <summary> + </summary> ✍ *" In this paper, we present Artemis - a visually aware, machine-learning-based IE method for heterogeneous visually rich documents. Artemis represents a visual span in a document by jointly encoding its visual and textual context for IE tasks. Our main contribution is two-fold. First, we develop a deep-learning model that identifies the local context boundary of a visual span with minimal human-labeling. Second, we describe a deep neural network that encodes the multimodal context of a visual span into a fixed-length vector by taking its textual and layout-specific features into account. It identifies the visual span(s) containing a named entity by leveraging this learned representation followed by an inference task. "* </details>
6. [CERES: Distantly Supervised Relation Extraction from the Semi-Structured Web](http://www.vldb.org/pvldb/vol11/p1084-lockard.pdf), Colin Lockard, Xin Luna Dong, Arash Einolghozati, and Prashant Shiralkar  ```VLDB 2018``` <details> <summary> + </summary> ✍ *" In this paper we present a new method for automatic extraction from semi-structured websites based on distant supervision. We automatically generate training labels by aligning an existing knowledge base with a website and leveraging the unique structural characteristics of semi-structured websites. We then train a classifier based on the potentially noisy and incomplete labels to predict new relation instances. "* </details>
7. [Glean: Structured Extractions from Templatic Documents](http://www.vldb.org/pvldb/vol14/p997-tata.pdf), Sandeep Tata, Navneet Potti, James B. Wendt, Lauro Beltrão Costa, Marc Najork, and Beliz Gunel  ```VLDB 2021``` <details> <summary> + </summary> ✍ *" Given a target schema for a document type and some labeled documents of that type, Glean uses machine learning to automatically extract structured information from other documents of that type. In this paper, we describe the overall architecture of Glean, and discuss three key data management challenges : 1) managing the quality of ground truth data, 2) generating training data for the machine learning model using labeled documents, and 3) building tools that help a developer rapidly build and improve a model for a given document type.  "* </details>
8. [FreeDOM: A Transferable Neural Architecture for Structured Information Extraction on Web Documents](https://dl.acm.org/doi/pdf/10.1145/3394486.3403153), Bill Yuchen Lin, Ying Sheng, Nguyen Vo, and Sandeep Tata  ```SIGKDD 2020``` <details> <summary> + </summary> ✍ *" Extracting structured data from HTML documents is a long-studied problem with a broad range of applications like augmenting knowledge bases, supporting faceted search, and providing domain-specific experiences for key verticals like shopping and movies. Previous approaches have either required a small number of examples for each target site or relied on carefully handcrafted heuristics built over visual renderings of websites. In this paper, we present a novel two-stage neural approach, named FreeDOM, which overcomes
both these limitations. The first stage learns a representation for each DOM node in the page by combining both the text and markup information. The second stage captures longer range distance and semantic relatedness using a relational neural network. By combining these stages, FreeDOM is able to generalize to unseen sites after training on a small number of seed sites from that vertical without requiring expensive hand-crafted features over visual renderings of the page. "* </details>
7. [One-shot Text Field Labeling using Attention and Belief Propagation for Structure Information Extraction](https://dl.acm.org/doi/pdf/10.1145/3394171.3413511), Mengli Cheng, Minghui Qiu, Xing Shi, Jun Huang, and Wei Lin  ```MM 2020``` <details> <summary> + </summary> ✍ *" Existing learning based methods for text labeling task usually require a large amount of labeled examples to train a specific model for each type of document. However, collecting large amounts of document images and labeling them is difficult and sometimes impossible due to privacy issues. Deploying separate models for each type of document also consumes a lot of resources. Facing these challenges, we explore one-shot learning for the text field labeling task. Existing one-shot learning methods for the task are mostly rule-based and have difficulty in labeling fields in crowded regions with few landmarks and fields consisting of multiple separate text regions. To alleviate
these problems, we proposed a novel deep end-to-end trainable approach for one-shot text field labeling, which makes use of attention mechanism to transfer the layout information between document images. We further applied conditional random field on the transferred layout information for the refinement of field labeling. "* </details>
8. [Combining visual and textual features for information extraction from online flyers](https://www.aclweb.org/anthology/D14-1206.pdf), Emilia Apostolova, Noriko Tomuro  ```EMNLP 2014``` <details> <summary> + </summary> ✍ *" Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features. In particular, genres such as marketing flyers and info-graphics often augment textual information by its color, size, positioning, etc. As a result, traditional text-based approaches to information extraction (IE) could underperform. In this study, we present a supervised machine learning approach to IE from online commercial real estate flyers. We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features. "* </details>
9. [Chargrid: Towards Understanding 2D Documents](https://www.aclweb.org/anthology/D18-1476.pdf), Anoop R Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen Bickel, Johannes Hohne, and Jean Baptiste Faddoul  ```EMNLP 2018``` <details> <summary> + </summary> ✍ *" We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation, we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a segmentation mask and bounding boxes. "* </details>
10. [Graph Convolution for Multimodal Information Extraction from Visually Rich Documents](https://www.aclweb.org/anthology/N19-2005.pdf), Xiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha Zhao  ```NAACL 2019``` <details> <summary> + </summary> ✍ *" In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. "* </details>
11. [Zeroshotceres: Zero-shot relation extraction from semi-structured webpages](https://www.aclweb.org/anthology/2020.acl-main.721.pdf), Colin Lockard, Prashant Shiralkar, Xin Luna Dong, Hannaneh Hajishirzi  ```ACL 2020``` <details> <summary> + </summary> ✍ *" In this work, we propose a solution for “zero-shot” open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals. Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates. "* </details>
12. [Representation learning for information extraction from form-like documents](https://www.aclweb.org/anthology/2020.acl-main.580.pdf), Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley Wendt, Qi Zhao, Marc Najork  ```ACL 2020``` <details> <summary> + </summary> ✍ *" We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates, and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document. "* </details>
13. [End-to-End Extraction of Structured Information from Business Documents with Pointer-Generator Networks](https://www.aclweb.org/anthology/2020.spnlp-1.6.pdf), Clément Sage, Alex Aussem, Véronique Eglin, Haytham Elghazel, and Jérémy Espinas  ```Workshop on Structured Prediction, ACL 2020``` <details> <summary> + </summary> ✍ *" The predominant approaches for extracting key information from documents resort to classifiers predicting the information type of each word. However, the word level ground truth used for learning is expensive to obtain since it is not naturally produced by the extraction task. In this paper, we discuss a new method for training extraction models directly from the textual value of information. The extracted information of a document is represented as a sequence of tokens in the XML language. We learn to output this representation with a pointer-generator network that alternately copies the document words carrying information and generates the XML tags delimiting the types of information. "* </details>
14. [Robust Layout-aware IE for Visually Rich Documents with Pre-trained Language Models](https://dl.acm.org/doi/pdf/10.1145/3397271.3401442), Mengxi Wei, Yifan He, and Qiong Zhang  ```SIGIR 2020``` <details> <summary> + </summary> ✍ *" We study the problem of information extraction from visually rich documents (VRDs) and present a model that combines the power of large pre-trained language models and graph neural networks to efficiently encode both textual and visual information in business documents. We further introduce new fine-tuning objectives to improve in-domain unsupervised fine-tuning to better utilize large amount of unlabeled in-domain data. "* </details>
15. [BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding](https://openreview.net/pdf?id=H1gsGaq9US), Timo I. Denk, Christian Reisswig  ```Workshop on Document Intelligence, NeurIPS 2019``` <details> <summary> + </summary> ✍ *" Our novel BERTgrid, which is based on Chargrid by Katti et al. (2018), represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network. The contextualized embedding vectors are retrieved from a BERT language model. We use BERTgrid in combination with a fully convolutional network on a semantic instance segmentation task for extracting fields from invoices. "* </details>
16. [PICK: Processing Key Information Extraction from Documents using Improved Graph Learning-Convolutional Networks](https://arxiv.org/pdf/2004.07464.pdf), Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, and Rong Xiao  ```*Pre-print*``` <details> <summary> + </summary> ✍ *" In this paper, we introduce PICK, a framework that is effective and robust in handling complex documents layout for Key Information Extraction by combining graph learning with graph convolution operation, yielding a richer semantic representation containing the textual and visual features and global layout without ambiguity. "* </details>
17. [Spatial Dependency Parsing for Semi-Structured Document Information Extraction](https://arxiv.org/pdf/2005.00642.pdf), Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang, and Minjoon Seo  ```*Pre-print*``` <details> <summary> + </summary> ✍ *" Information Extraction (IE) for semi-structured document images is often approached as a sequence tagging problem by classifying each recognized input token into one of the IOB (Inside, Outside, and Beginning) categories. However, such problem setup has two inherent limitations that (1) it cannot easily handle complex spatial relationships and (2) it is not suitable for highly structured information, which are nevertheless frequently observed in real-world document images. To tackle these issues, we first formulate the IE task as spatial dependency parsing problem that focuses on the relationship among text segment nodes in the documents. Under this setup, we then propose SPADE (SPAtial DEpendency parser) that models highly complex spatial relationships and an arbitrary number of information layers in the documents in an end-to-end manner. "* </details>
18. [Abstractive Information Extraction from Scanned Invoices (AIESI) using End-to-end Sequential Approach](https://arxiv.org/abs/2009.05728), Shreeshiv Patel, Dvijesh Bhatt  ```*Pre-print*``` <details> <summary> + </summary> ✍ *" Abstract Information Extraction from Scanned Invoices (AIESI) is a process of extracting information like, date, total amount, payee name, and etc from scanned receipts. In this paper we proposed an improved method to ensemble all visual and textual features from invoices to extract key invoice parameters using Word wise BiLSTM. "* </details>
19. [VisualWordGrid: Information Extraction From Scanned Documents Using A Multimodal Approach](https://arxiv.org/pdf/2010.02358.pdf), Mohamed Kerroumi, Othmane Sayem, and Aymen Shabou  ```*Pre-print*``` <details> <summary> + </summary> ✍ *" We introduce a novel approach for scanned document representation to perform field extraction. It allows the simultaneous encoding of the textual, visual and layout information in a 3D matrix used as an input to a segmentation model. We improve the recent Chargrid and Wordgrid models in several ways, first by taking into account the visual modality, then by boosting its robustness in regards to small datasets while keeping the inference time low. "* </details>
20. [Wordgrid: Extending Chargrid with Word-level Information](https://www.researchgate.net/profile/Timo-Denk/publication/335715433_Wordgrid_Extending_Chargrid_with_Word-level_Information/links/5d77604c92851cacdb2e0858/Wordgrid-Extending-Chargrid-with-Word-level-Information.pdf), Timo I. Denk  ```*Pre-print*``` <details> <summary> + </summary> ✍ *" For embedding words with semantically meaningful vectors, we propose
a novel method for estimating dense word vectors, called word2vec-2d. It is a fork of word2vec that is trained on 2D document corpora rather than 1D text sequences. The notion of context is redefined to be the variablysized set of words that are spatially located within a certain distance to the center word. "* </details>
21. [Spatial Dual-Modality Graph Reasoning for Key Information Extraction](https://arxiv.org/pdf/2103.14470.pdf), Hongbin Sun, Zhanghui Kuang, Xiaoyu Yue, Chenhao Lin and Wayne Zhang ```*Pre-print*``` <details> <summary> + </summary> ✍ *" In this paper, we propose an end-toend Spatial Dual-Modality Graph Reasoning method (SDMGR) to extract key information from unstructured document images. We model document images as dual-modality graphs, nodes of which encode both the visual and textual features of detected text regions, and edges of which represent the spatial relations between neighboring text regions. The key information extraction is solved by iteratively propagating messages along graph edges and reasoning the categories of graph nodes. In order to roundly evaluate our proposed method as well as boost the future research, we release a new dataset named WildReceipt, which is collected and annotated tailored for the evaluation of key information extraction from document images of unseen templates in the wild. "* </details>

<br/>

# Language Model

1. [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://dl.acm.org/doi/abs/10.1145/3394486.3403172), Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou ```SIGKDD 2020``` <details> <summary> + </summary> ✍ *" In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. "* </details>
2.  [Self-Supervised Representation Learning on Document Images](https://arxiv.org/pdf/2004.10605.pdf), Adrian Cosma, Mihai Ghidoveanu, Michael Panaitescu-Liess, and Marius Popescu ```Workshop on Document Analysis Systems, 2020``` <details> <summary> + </summary> ✍ *" This work analyses the impact of self-supervised pre-training on document images in the context of document image classification. While previous approaches explore the effect of self-supervision on natural images, we show that patch-based pre-training performs poorly on document images because of their different structural properties and poor intra-sample semantic information. We propose two context-aware alternatives to improve performance on the Tobacco-3482 image classification task. We also propose a novel method for self-supervision, which makes use of the inherent multi-modality of documents (image and text). "* </details>
3. [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/pdf/2012.14740.pdf), Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou  ```*Pre-print*``` <details> <summary> + </summary> ✍ *" In this paper, we present LayoutLMv2 by pre-training text, layout and image in a multi-modal framework, where new model architectures and pre-training tasks are leveraged. Specifically, LayoutLMv2 not only uses the existing masked visual-language modeling task but also the new text-image alignment and textimage matching tasks in the pre-training stage, where cross-modality interaction is better learned. Meanwhile, it also integrates a spatial-aware selfattention mechanism into the Transformer architecture, so that the model can fully understand the relative positional relationship among different text blocks. "* </details>
4. [LAMBERT: Layout-Aware (Language) Modeling for information extraction](https://arxiv.org/pdf/2002.08087.pdf), Łukasz Garncarek, Rafał Powalski, Tomasz Stanisławek, Bartosz Topolski, Piotr Halama, Michał Turski, and Filip Graliński  ```*Pre-print*``` <details> <summary> + </summary> ✍ *" We introduce a new simple approach to the problem of understanding documents where non-trivial layout influences the local semantics. To this end, we modify the Transformer encoder architecture in a way that allows it to use layout features obtained from an OCR system, without the need to re-learn the language semantics from scratch. We augment the input of the model only with the coordinates of token bounding boxes, avoiding the use of raw images. This leads to a layout-aware language model which can be then fine-tuned on downstream tasks. "* </details>
5. [Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning](https://arxiv.org/pdf/2009.14457), Subhojeet Pramanik, Shashank Mujumdar, Hima Patel  ```*Pre-print*``` <details> <summary> + </summary> ✍ *" In this paper, we propose a multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre-training tasks to learn a generic document representation. We design the network architecture and the pretraining tasks to incorporate the multi-modal document information across text, layout, and image dimensions and allow the network to work with multi-page documents. We showcase the applicability of our pre-training framework on a variety of different real-world document tasks. "* </details>
6. [LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding](https://arxiv.org/pdf/2104.08405.pdf), Te-Lin Wu, Cheng Li, Mingyang Zhan, Tao Chen, Spurthi Amba Hombaiah, Michael Bendersky  ```*Pre-print*``` <details> <summary> + </summary> ✍ *" We parse a document into content blocks (e.g. text, table, image) and propose a novel layout-aware multimodal hierarchical framework, LAMPreT, to model the blocks and the whole document. Our LAMPreT encodes each block with a multimodal transformer in the lower-level, and aggregates the block-level representations and connections utilizing a specifically designed transformer at the higher-level. We design hierarchical pretraining objectives where the lower-level model is trained with the standard masked language modeling (MLM) loss and the image-text matching loss, and the higher-level model is trained with three layout-aware objectives: (1) block-order predictions, (2) masked block predictions, and (3) image fitting predictions. "* </details>
7. [Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer](https://arxiv.org/pdf/2102.09550.pdf), Rafał Powalski, Łukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Michał Pietruszka, and Gabriela Pałka  ```*Pre-print*``` <details> <summary> + </summary> ✍ *" We address the challenging problem of Natural Language Comprehension beyond plain-text documents by introducing the TILT neural network architecture which simultaneously learns layout information, visual features, and textual semantics. Contrary to previous approaches, we rely on a decoder capable of unifying a variety of problems involving natural language. The layout is represented as an attention bias and complemented with contextualized visual information, while the core of our model is a pretrained encoder-decoder Transformer. "* </details>

<br/>

# Layout Analysis

1. [Form2Seq : A Framework for Higher-Order Form Structure Extraction](https://www.aclweb.org/anthology/2020.emnlp-main.314.pdf), Milan Aggarwal, Hiresh Gupta, Mausoom Sarkar, Balaji Krishnamurthy ```EMNLP 2020``` 
2. [DocStruct: A Multimodal Method to Extract Hierarchy Structure in Document for General Form Understanding](https://www.aclweb.org/anthology/2020.findings-emnlp.80.pdf), Zilong Wang, Mingjie Zhan, Xuebo Liu, and Ding Liang  ```EMNLP Findings 2020``` 
3. [Multi-Modal Association based Grouping for Form Structure Extraction](https://openaccess.thecvf.com/content_WACV_2020/papers/Aggarwal_Multi-Modal_Association_based_Grouping_for_Form_Structure_Extraction_WACV_2020_paper.pdf), Milan Aggarwal, Mausoom Sarkar, Hiresh Gupta, and Balaji Krishnamurthy ```WACV 2020``` 
4. [Simplified DOM Trees for Transferable Attribute Extraction from the Web](https://arxiv.org/pdf/2101.02415), Yichao Zhou, Ying Sheng, Nguyen Vo, Nick Edmonds, and Sandeep Tata  ```The Web Conference, 2021```
5. [Few-shot prototype alignment regularization network for document image layout segementation](https://www.sciencedirect.com/science/article/pii/S0031320321000698?casa_token=uVU_XJ_PiRMAAAAA:YWsrBPcRROXebLkjl04a1uNnWZxwJmTEOF8woqF9Td5KIhjgOIgLpV0Hlv18jEZF1S7fmegLZYjk), Yujie Li, Pengfei Zhang, Xing Xua, Yi Lai, Fumin Shen, Lijiang Chen, and Pengxiang Gao ```Pattern Recognition 2021```
6. [Vips: a vision-based page segmentation algorithm](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2003-79.doc), Deng Cai, Shipeng Yu, Ji-Rong Wen, and Wei-Ying Ma ```Technical Report```
7. [DocParser: Hierarchical Document Structure Parsing from Renderings](https://arxiv.org/pdf/1911.01702.pdf), Johannes Rausch, Octavio Martinez, Fabian Bissig,
Ce Zhang, and Stefan Feuerriege ```*Pre-print*```

<br/>

# Classification

1. [Deterministic Routing between Layout Abstractions for Multi-Scale classification of Visually Rich Documents](https://www.ijcai.org/Proceedings/2019/0466.pdf), Ritesh Sarkhel, Arnab Nandi  ```IJCAI 2019``` 
2. [Cutting the error by half: Investigation of very deep cnn and advanced training strategies for document image classification](https://ieeexplore.ieee.org/document/8270080), Muhammad Zeshan Afzal, Andreas Kölsch, Sheraz Ahmed, and Marcus Liwicki  ```ICDAR 2017``` 
3. [Real-time document image classification using deep CNN and extreme learning machines](https://ieeexplore.ieee.org/document/8270148), Andreas Kölsch, Muhammad Zeshan Afzal, Markus Ebbecke, and Marcus Liwicki ```ICDAR 2017``` 
4. [Analysis of convolutional neural networks for document image classification](https://ieeexplore.ieee.org/document/8270002), Chris Tensmeyer, Tony Martinez  ```ICDAR 2017```
5.  [Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks](https://ieeexplore.ieee.org/document/8545630), Arindam Das, Saikat Roy, Ujjwal Bhattacharya, and Swapan K. Parui  ```ICPR 2018```
6. [Multimodal Document Image Classification](https://ieeexplore.ieee.org/document/8977998), Rajiv Jain, Curtis Wigington  ```ICDAR 2019```
7. [Two Stream Deep Network for Document Image Classification](https://ieeexplore.ieee.org/document/8978000), Muhammad Nabeel Asim, Muhammad Usman Ghani Khan, Muhammad Imran Malik, Khizar Razzaque, Andreas Dengel, and Sheraz Ahmed ```ICDAR 2019```
8. [Unsupervised exemplar-based learning for improved document image classification](https://ieeexplore.ieee.org/iel7/6287639/8600701/08843852.pdf), Sherif Abuelwafa, Marco Pedersoli, and Mohamed Cheriet ```IEEE Access 2019```
9. [Visual and Textual Deep Feature Fusion for Document Image Classification](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w34/Bakkali_Visual_and_Textual_Deep_Feature_Fusion_for_Document_Image_Classification_CVPRW_2020_paper.pdf), Souhail Bakkali, Zuheng Ming, Mickael Coustaty, and Marc al Rusinol ```CVPR Workshop 2020```
10. [Structural similarity for document image classification and retrieval](https://www.sciencedirect.com/science/article/pii/S0167865513004224?casa_token=S_v8V3Ad8_sAAAAA:w7K-TImQVxvilo8QbK1kf0Jr4iggP5YVZfoB9dpWYcBGl53LXiL9Zynmi4uVp13gD-jxuJLySD_f), Jayant Kumar, Peng Ye, and David Doermann ```Pattern Recognition Letters 2014```

<br/>

# Resource Papers

1. [DocBank: A Benchmark Dataset for Document Layout Analysis](https://www.aclweb.org/anthology/2020.coling-main.82.pdf), Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou  ```COLING 2020```
2. [Evaluation of deep convolutional nets for document image classification and retrieval](https://ieeexplore.ieee.org/iel7/7321714/7333702/07333910.pdf?casa_token=ghPOsBUaz5gAAAAA:hn5TYNmd9mQimKG5Uhsp87DXBZlvYFnjE5OWJhmqSppKZ-GOWQVAMzGU8xEZxBSbeM9Ak54hI1g), Muhammad Zeshan Afzal, Andreas Kölsch, Sheraz Ahmed, and Marcus Liwicki ```ICDAR 2015```
3. [ICDAR2017 Competition on Layout Analysis for Challenging Medieval Manuscripts](https://ieeexplore.ieee.org/document/8270154), Fotini Simistira, Manuel Bouillon, Mathias Seuret, Marcel Würsch, Michele Alberti, Rolf Ingold, and Marcus Liwicki ```ICDAR 2017```
4. [DocVQA: A Dataset for VQA on Document Images](https://openaccess.thecvf.com/content/WACV2021/papers/Mathew_DocVQA_A_Dataset_for_VQA_on_Document_Images_WACV_2021_paper.pdf), Minesh Mathew1 Dimosthenis Karatzas, and C.V. Jawahar ```WACV 2021```
5. [ICDAR2019 competition on scanned receipt ocr and information extraction](https://ieeexplore.ieee.org/iel7/8961318/8977948/08977955.pdf?casa_token=VZyP-jSdoxEAAAAA:_MwdC_TF05d6LjGhTjj8BpTOidioxoW4AC7m6MMpaDDTvlFEYD3OTjc93KcQbNNUZyk14ai_bCk), Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and C. V. Jawahar ```ICDAR 2019```
6. [FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents](https://ieeexplore.ieee.org/document/8892998), Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran  ```ICDAR Workshop 2019```
7. [Kleister: A novel task for Information Extraction involving Long Documents with Complex Layout](https://arxiv.org/pdf/2003.02356.pdf), Filip Graliński, Tomasz Stanisławek, Anna Wróblewska, Dawid Lipiński, Agnieszka Kaliska, Paulina Rosalska, Bartosz Topolski, and Przemysław Biecek  ```*Pre-print*```
8. [WebSRC: A Dataset for Web-Based Structural Reading Comprehension](https://arxiv.org/pdf/2101.09465), Lu Chen, Xingyu Chen, Zihan Zhao, Danyang Zhang
Jiabao Ji, Ao Luo, Yuxuan Xiong, and Kai Yu  ```*Pre-print*```
9. [VisualMRC: Machine Reading Comprehension on Document Images](https://arxiv.org/pdf/2101.11272.pdf), Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida  ```*Pre-print*```
