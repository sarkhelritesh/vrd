# A Snapshot of the Current Task-Space for Visually Rich Documents

### [Ritesh Sarkhel](https://sarkhelritesh.github.io/)

<br/>

![Sample Visually Rich Documents](https://github.com/sarkhelritesh/vrd_resource/blob/main/github_vrd_example.jpg?raw=true "Sample Visually Rich Documents")

A lot of interesting works have been published on Visually Rich Documents (VRD) in recent years. What makes tasks defined on a VRD often more challenging than its plain-text counterpart is its multimodal nature. We provide a list of works on various problems related to VRD here. Our hope is that this will act as a *resource of reference* for researchers interested in this domain. The following list presents, in no particular order, a number of intersting papers on VRD published in recent years. You will also find a bibtex file (```vrd.bib```) in this repository containing all the papers listed below.

<br/>

> **Disclaimer:** Please note that this is \*\*not\*\* an exhaustive list. If you think a work should feature on this list, please submit a pull request or [email me](mailto:sarkhel.5@osu.edu). We look forward to your contribution to keep this page up-to-date and useful.

<br/>

# Content Overview

We list these papers in five different directions. 

- [Information Extraction](#information-extraction)
- [Language Model](#language-model)
- [Layout Analysis](#layout-analysis)
- [Classification](#document-classification)
- [Resource Papers](#resource-papers)

<br/>

# Information Extraction

1. [Fonduer: Knowledge base construction from richly formatted data](https://dl.acm.org/doi/pdf/10.1145/3183713.3183729), Sen Wu, Luke Hsiao, Xiao Cheng, Braden Hancock, Theodoros Rekatsinas, Philip Levis, and Christopher Ré ```SIGMOD 2018``` <details> <summary> + </summary> *"We focus on knowledge base construction (KBC) from richly formatted data. In contrast to KBC from text or tabular data, KBC from richly formatted data aims to extract relations conveyed jointly via textual, structural, tabular, and visual expressions. We introduce Fonduer, a machine-learning-based KBC system for richly formatted data. Fonduer presents a new data model that accounts for three challenging characteristics of richly formatted data: (1) prevalent document-level relations, (2) multimodality, and (3) data variety. Fonduer uses a new deep-learning model to automatically capture the representation (i.e., features) needed to learn how to extract relations from richly formatted data. Finally, Fonduer provides a new programming model that enables users to convert domain expertise, based on multiple modalities of information, to meaningful signals of supervision for training a KBC system"* </details> 
2. [Visual segmentation for information extraction from heterogeneous visually rich documents](https://dl.acm.org/doi/pdf/10.1145/3299869.3319867), Ritesh Sarkhel, Arnab Nandi  ```SIGMOD 2019``` <details> <summary> + </summary> *" We propose VS2, a generalized approach for information extraction from heterogeneous visually rich documents. There are two major contributions of this work. First, we propose a robust segmentation algorithm that decomposes a visually rich document into a bag of visually isolated but semantically coherent areas, called logical blocks. Document type agnostic low-level visual and semantic features are used in this process. Our second contribution is a distantly supervised search-and-select method for identifying the named entities within these documents by utilizing the context boundaries defined by these logical blocks."* </details>
3. [Improving Information Extraction from Visually Rich Documents using Visual Span Representations](https://dl.acm.org/doi/pdf/10.14778/3446095.3446104), Ritesh Sarkhel, Arnab Nandi  ```VLDB 2021``` <details> <summary> + </summary> *" In this paper, we present Artemis - a visually aware, machine-learning-based IE method for heterogeneous visually rich documents. Artemis represents a visual span in a document by jointly encoding its visual and textual context for IE tasks. Our main contribution is two-fold. First, we develop a deep-learning model that identifies the local context boundary of a visual span with minimal human-labeling. Second, we describe a deep neural network that encodes the multimodal context of a visual span into a fixed-length vector by taking its textual and layout-specific features into account. It identifies the visual span(s) containing a named entity by leveraging this learned representation followed by an inference task. "* </details>
4. [CERES: Distantly Supervised Relation Extraction from the Semi-Structured Web](http://www.vldb.org/pvldb/vol11/p1084-lockard.pdf), Colin Lockard, Xin Luna Dong, Arash Einolghozati, and Prashant Shiralkar  ```VLDB 2018``` <details> <summary> + </summary> *" In this paper we present a new method for automatic extraction from semi-structured websites based on distant supervision. We automatically generate training labels by aligning an existing knowledge base with a website and leveraging the unique structural characteristics of semi-structured websites. We then train a classifier based on the potentially noisy and incomplete labels to predict new relation instances. "* </details>
5. [Glean: Structured Extractions from Templatic Documents](http://www.vldb.org/pvldb/vol14/p997-tata.pdf), Sandeep Tata, Navneet Potti, James B. Wendt, Lauro Beltrão Costa, Marc Najork, and Beliz Gunel  ```VLDB 2021``` <details> <summary> + </summary> *" Given a target schema for a document type and some labeled documents of that type, Glean uses machine learning to automatically extract structured information from other documents of that type. In this paper, we describe the overall architecture of Glean, and discuss three key data management challenges : 1) managing the quality of ground truth data, 2) generating training data for the machine learning model using labeled documents, and 3) building tools that help a developer rapidly build and improve a model for a given document type.  "* </details>
6. [FreeDOM: A Transferable Neural Architecture for Structured Information Extraction on Web Documents](https://dl.acm.org/doi/pdf/10.1145/3394486.3403153), Bill Yuchen Lin, Ying Sheng, Nguyen Vo, and Sandeep Tata  ```SIGKDD 2020``` <details> <summary> + </summary> *" Extracting structured data from HTML documents is a long-studied problem with a broad range of applications like augmenting knowledge bases, supporting faceted search, and providing domain-specific experiences for key verticals like shopping and movies. Previous approaches have either required a small number of examples for each target site or relied on carefully handcrafted heuristics built over visual renderings of websites. In this paper, we present a novel two-stage neural approach, named FreeDOM, which overcomes
both these limitations. The first stage learns a representation for each DOM node in the page by combining both the text and markup information. The second stage captures longer range distance and semantic relatedness using a relational neural network. By combining these stages, FreeDOM is able to generalize to unseen sites after training on a small number of seed sites from that vertical without requiring expensive hand-crafted features over visual renderings of the page. "* </details>
7. [One-shot Text Field Labeling using Attention and Belief Propagation for Structure Information Extraction](https://dl.acm.org/doi/pdf/10.1145/3394171.3413511), Mengli Cheng, Minghui Qiu, Xing Shi, Jun Huang, and Wei Lin  ```MM 2020``` <details> <summary> + </summary> *" Existing learning based methods for text labeling task usually require a large amount of labeled examples to train a specific model for each type of document. However, collecting large amounts of document images and labeling them is difficult and sometimes impossible due to privacy issues. Deploying separate models for each type of document also consumes a lot of resources. Facing these challenges, we explore one-shot learning for the text field labeling task. Existing one-shot learning methods for the task are mostly rule-based and have difficulty in labeling fields in crowded regions with few landmarks and fields consisting of multiple separate text regions. To alleviate
these problems, we proposed a novel deep end-to-end trainable approach for one-shot text field labeling, which makes use of attention mechanism to transfer the layout information between document images. We further applied conditional random field on the transferred layout information for the refinement of field labeling. "* </details>
8. [Combining visual and textual features for information extraction from online flyers](https://www.aclweb.org/anthology/D14-1206.pdf), Emilia Apostolova, Noriko Tomuro  ```EMNLP 2014``` <details> <summary> + </summary> *" Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features. In particular, genres such as marketing flyers and info-graphics often augment textual information by its color, size, positioning, etc. As a result, traditional text-based approaches to information extraction (IE) could underperform. In this study, we present a supervised machine learning approach to IE from online commercial real estate flyers. We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features. "* </details>
9. [Chargrid: Towards Understanding 2D Documents](https://www.aclweb.org/anthology/D18-1476.pdf), Anoop R Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen Bickel, Johannes Hohne, and Jean Baptiste Faddoul  ```EMNLP 2018``` <details> <summary> + </summary> *" We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation, we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a segmentation mask and bounding boxes. "* </details>
10. [Graph Convolution for Multimodal Information Extraction from Visually Rich Documents](https://www.aclweb.org/anthology/N19-2005.pdf), Xiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha Zhao  ```NAACL 2019``` <details> <summary> + </summary> *" In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. "* </details>
11. [Zeroshotceres: Zero-shot relation extraction from semi-structured webpages](https://www.aclweb.org/anthology/2020.acl-main.721.pdf), Colin Lockard, Prashant Shiralkar, Xin Luna Dong, Hannaneh Hajishirzi  ```ACL 2020``` <details> <summary> + </summary> *" In this work, we propose a solution for “zero-shot” open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals. Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates. "* </details>
12. [Representation learning for information extraction from form-like documents](https://www.aclweb.org/anthology/2020.acl-main.580.pdf), Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley Wendt, Qi Zhao, Marc Najork  ```ACL 2020```
13. [End-to-End Extraction of Structured Information from Business Documents with Pointer-Generator Networks](https://www.aclweb.org/anthology/2020.spnlp-1.6.pdf), Clément Sage, Alex Aussem, Véronique Eglin, Haytham Elghazel, and Jérémy Espinas  ```Workshop on Structured Prediction, ACL 2020``` 
14. [Robust Layout-aware IE for Visually Rich Documents with Pre-trained Language Models](https://dl.acm.org/doi/pdf/10.1145/3397271.3401442?casa_token=U2oKgMPm_AoAAAAA:AOSntcj2iIExwYnjGIpRQOjE8e2VAWBo_-_EHGdUkHDqUZoKtAHmg4gdXbOJpQBDpBQh5SYzyRQjRlU), Mengxi Wei, Yifan He, and Qiong Zhang  ```SIGIR 2020```
15. [BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding](https://openreview.net/pdf?id=H1gsGaq9US), Timo I. Denk, Christian Reisswig  ```Workshop on Document Intelligence, NeurIPS 2019``` 
16. [PICK: Processing Key Information Extraction from Documents using Improved Graph Learning-Convolutional Networks](https://arxiv.org/pdf/2004.07464.pdf), Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, and Rong Xiao  ```*Pre-print*``` 
17. [Spatial Dependency Parsing for Semi-Structured Document Information Extraction](https://arxiv.org/pdf/2005.00642.pdf), Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang, and Minjoon Seo  ```*Pre-print*```
18. [Abstractive Information Extraction from Scanned Invoices (AIESI) using End-to-end Sequential Approach](https://arxiv.org/abs/2009.05728), Shreeshiv Patel, Dvijesh Bhatt  ```*Pre-print*```
19. [VisualWordGrid: Information Extraction From Scanned Documents Using A Multimodal Approach](https://arxiv.org/pdf/2010.02358.pdf), Mohamed Kerroumi, Othmane Sayem, and Aymen Shabou  ```*Pre-print*``` 
20. [Wordgrid: Extending Chargrid with Word-level Information](https://www.researchgate.net/profile/Timo-Denk/publication/335715433_Wordgrid_Extending_Chargrid_with_Word-level_Information/links/5d77604c92851cacdb2e0858/Wordgrid-Extending-Chargrid-with-Word-level-Information.pdf), Timo I. Denk  ```*Pre-print*``` 
21. [Spatial Dual-Modality Graph Reasoning for Key Information Extraction](https://arxiv.org/pdf/2103.14470.pdf), Hongbin Sun, Zhanghui Kuang, Xiaoyu Yue, Chenhao Lin and Wayne Zhang ```*Pre-print*```

<br/>

# Language Model

1. [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://dl.acm.org/doi/abs/10.1145/3394486.3403172), Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou ```SIGKDD 2020```
2.  [Self-Supervised Representation Learning on Document Images](https://arxiv.org/pdf/2004.10605.pdf), Adrian Cosma, Mihai Ghidoveanu, Michael Panaitescu-Liess, and Marius Popescu ```Workshop on Document Analysis Systems, 2020```
3. [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/pdf/2012.14740.pdf), Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou  ```*Pre-print*```
4. [LAMBERT: Layout-Aware (Language) Modeling for information extraction](https://arxiv.org/pdf/2002.08087.pdf), Łukasz Garncarek, Rafał Powalski, Tomasz Stanisławek, Bartosz Topolski, Piotr Halama, Michał Turski, and Filip Graliński  ```*Pre-print*```
5. [Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning](https://arxiv.org/pdf/2009.14457), Subhojeet Pramanik, Shashank Mujumdar, Hima Patel  ```*Pre-print*```
6. [LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding](https://arxiv.org/pdf/2104.08405.pdf), Te-Lin Wu, Cheng Li, Mingyang Zhan, Tao Chen, Spurthi Amba Hombaiah, Michael Bendersky  ```*Pre-print*```
7. [Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer](https://arxiv.org/pdf/2102.09550.pdf), Rafał Powalski, Łukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Michał Pietruszka, and Gabriela Pałka  ```*Pre-print*```

<br/>

# Layout Analysis

1. [Form2Seq : A Framework for Higher-Order Form Structure Extraction](https://www.aclweb.org/anthology/2020.emnlp-main.314.pdf), Milan Aggarwal, Hiresh Gupta, Mausoom Sarkar, Balaji Krishnamurthy ```EMNLP 2020``` 
2. [DocStruct: A Multimodal Method to Extract Hierarchy Structure in Document for General Form Understanding](https://www.aclweb.org/anthology/2020.findings-emnlp.80.pdf), Zilong Wang, Mingjie Zhan, Xuebo Liu, and Ding Liang  ```EMNLP Findings 2020``` 
3. [Multi-Modal Association based Grouping for Form Structure Extraction](https://openaccess.thecvf.com/content_WACV_2020/papers/Aggarwal_Multi-Modal_Association_based_Grouping_for_Form_Structure_Extraction_WACV_2020_paper.pdf), Milan Aggarwal, Mausoom Sarkar, Hiresh Gupta, and Balaji Krishnamurthy ```WACV 2020``` 
4. [Simplified DOM Trees for Transferable Attribute Extraction from the Web](https://arxiv.org/pdf/2101.02415), Yichao Zhou, Ying Sheng, Nguyen Vo, Nick Edmonds, and Sandeep Tata  ```The Web Conference, 2021```
5. [Few-shot prototype alignment regularization network for document image layout segementation](https://www.sciencedirect.com/science/article/pii/S0031320321000698?casa_token=uVU_XJ_PiRMAAAAA:YWsrBPcRROXebLkjl04a1uNnWZxwJmTEOF8woqF9Td5KIhjgOIgLpV0Hlv18jEZF1S7fmegLZYjk), Yujie Li, Pengfei Zhang, Xing Xua, Yi Lai, Fumin Shen, Lijiang Chen, and Pengxiang Gao ```Pattern Recognition 2021```
6. [Vips: a vision-based page segmentation algorithm](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2003-79.doc), Deng Cai, Shipeng Yu, Ji-Rong Wen, and Wei-Ying Ma ```Technical Report```
7. [DocParser: Hierarchical Document Structure Parsing from Renderings](https://arxiv.org/pdf/1911.01702.pdf), Johannes Rausch, Octavio Martinez, Fabian Bissig,
Ce Zhang, and Stefan Feuerriege ```*Pre-print*```

<br/>

# Classification

1. [Deterministic Routing between Layout Abstractions for Multi-Scale classification of Visually Rich Documents](https://www.ijcai.org/Proceedings/2019/0466.pdf), Ritesh Sarkhel, Arnab Nandi  ```IJCAI 2019``` 
2. [Cutting the error by half: Investigation of very deep cnn and advanced training strategies for document image classification](https://ieeexplore.ieee.org/document/8270080), Muhammad Zeshan Afzal, Andreas Kölsch, Sheraz Ahmed, and Marcus Liwicki  ```ICDAR 2017``` 
3. [Real-time document image classification using deep CNN and extreme learning machines](https://ieeexplore.ieee.org/document/8270148), Andreas Kölsch, Muhammad Zeshan Afzal, Markus Ebbecke, and Marcus Liwicki ```ICDAR 2017``` 
4. [Analysis of convolutional neural networks for document image classification](https://ieeexplore.ieee.org/document/8270002), Chris Tensmeyer, Tony Martinez  ```ICDAR 2017```
5.  [Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks](https://ieeexplore.ieee.org/document/8545630), Arindam Das, Saikat Roy, Ujjwal Bhattacharya, and Swapan K. Parui  ```ICPR 2018```
6. [Multimodal Document Image Classification](https://ieeexplore.ieee.org/document/8977998), Rajiv Jain, Curtis Wigington  ```ICDAR 2019```
7. [Two Stream Deep Network for Document Image Classification](https://ieeexplore.ieee.org/document/8978000), Muhammad Nabeel Asim, Muhammad Usman Ghani Khan, Muhammad Imran Malik, Khizar Razzaque, Andreas Dengel, and Sheraz Ahmed ```ICDAR 2019```
8. [Unsupervised exemplar-based learning for improved document image classification](https://ieeexplore.ieee.org/iel7/6287639/8600701/08843852.pdf), Sherif Abuelwafa, Marco Pedersoli, and Mohamed Cheriet ```IEEE Access 2019```
9. [Visual and Textual Deep Feature Fusion for Document Image Classification](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w34/Bakkali_Visual_and_Textual_Deep_Feature_Fusion_for_Document_Image_Classification_CVPRW_2020_paper.pdf), Souhail Bakkali, Zuheng Ming, Mickael Coustaty, and Marc al Rusinol ```CVPR Workshop 2020```
10. [Structural similarity for document image classification and retrieval](https://www.sciencedirect.com/science/article/pii/S0167865513004224?casa_token=S_v8V3Ad8_sAAAAA:w7K-TImQVxvilo8QbK1kf0Jr4iggP5YVZfoB9dpWYcBGl53LXiL9Zynmi4uVp13gD-jxuJLySD_f), Jayant Kumar, Peng Ye, and David Doermann ```Pattern Recognition Letters 2014```

<br/>

# Resource Papers

1. [DocBank: A Benchmark Dataset for Document Layout Analysis](https://www.aclweb.org/anthology/2020.coling-main.82.pdf), Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou  ```COLING 2020```
2. [Evaluation of deep convolutional nets for document image classification and retrieval](https://ieeexplore.ieee.org/iel7/7321714/7333702/07333910.pdf?casa_token=ghPOsBUaz5gAAAAA:hn5TYNmd9mQimKG5Uhsp87DXBZlvYFnjE5OWJhmqSppKZ-GOWQVAMzGU8xEZxBSbeM9Ak54hI1g), Muhammad Zeshan Afzal, Andreas Kölsch, Sheraz Ahmed, and Marcus Liwicki ```ICDAR 2015```
3. [ICDAR2017 Competition on Layout Analysis for Challenging Medieval Manuscripts](https://ieeexplore.ieee.org/document/8270154), Fotini Simistira, Manuel Bouillon, Mathias Seuret, Marcel Würsch, Michele Alberti, Rolf Ingold, and Marcus Liwicki ```ICDAR 2017```
4. [DocVQA: A Dataset for VQA on Document Images](https://openaccess.thecvf.com/content/WACV2021/papers/Mathew_DocVQA_A_Dataset_for_VQA_on_Document_Images_WACV_2021_paper.pdf), Minesh Mathew1 Dimosthenis Karatzas, and C.V. Jawahar ```WACV 2021```
5. [ICDAR2019 competition on scanned receipt ocr and information extraction](https://ieeexplore.ieee.org/iel7/8961318/8977948/08977955.pdf?casa_token=VZyP-jSdoxEAAAAA:_MwdC_TF05d6LjGhTjj8BpTOidioxoW4AC7m6MMpaDDTvlFEYD3OTjc93KcQbNNUZyk14ai_bCk), Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and C. V. Jawahar ```ICDAR 2019```
6. [FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents](https://ieeexplore.ieee.org/document/8892998), Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran  ```ICDAR Workshop 2019```
7. [Kleister: A novel task for Information Extraction involving Long Documents with Complex Layout](https://arxiv.org/pdf/2003.02356.pdf), Filip Graliński, Tomasz Stanisławek, Anna Wróblewska, Dawid Lipiński, Agnieszka Kaliska, Paulina Rosalska, Bartosz Topolski, and Przemysław Biecek  ```*Pre-print*```
8. [WebSRC: A Dataset for Web-Based Structural Reading Comprehension](https://arxiv.org/pdf/2101.09465), Lu Chen, Xingyu Chen, Zihan Zhao, Danyang Zhang
Jiabao Ji, Ao Luo, Yuxuan Xiong, and Kai Yu  ```*Pre-print*```
9. [VisualMRC: Machine Reading Comprehension on Document Images](https://arxiv.org/pdf/2101.11272.pdf), Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida  ```*Pre-print*```
